{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# load the MNIST dataset\n",
    "mnist_data = fetch_openml('mnist_784')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "print(mnist_data['data'].shape)\n",
    "print(mnist_data['target'].shape)\n",
    "\n",
    "X = mnist_data['data']\n",
    "y = mnist_data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000,)\n",
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/7)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "index = 2\n",
    "\n",
    "# plot an example\n",
    "def plot_example(X, y):\n",
    "    plt.imshow(X.reshape(28, 28), cmap='binary')\n",
    "    plt.axis(False)\n",
    "    plt.title(f\"{y}\")\n",
    "\n",
    "plot_example(X_train[index], y_train[index])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot a few examples\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "for i in range(4):\n",
    "    fig.add_subplot(2, 2, i+1)\n",
    "    plot_example(X_train[i], y_train[i])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def one_hot_encode(x, num_classes=10):\n",
    "    x_one_hot = x.astype(int).reshape(-1)\n",
    "    x_one_hot = np.eye(num_classes)[x_one_hot]\n",
    "    return x_one_hot\n",
    "\n",
    "# one-hot encode labels\n",
    "y_train_one_hot = one_hot_encode(y_train)\n",
    "y_test_one_hot = one_hot_encode(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the inputs\n",
    "X_train_normalized = X_train / 255.0\n",
    "X_test_normalized = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Sequential\n",
    "from layers import Dense\n",
    "\n",
    "# create the model\n",
    "model = Sequential([\n",
    "    Dense(16, input_shape=(784,), activation=\"softmax\"),\n",
    "    Dense(16, activation=\"softmax\"),\n",
    "    Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=2.3024\taccuracy=0.1119\n",
      "(2.3024019977037873, [0.11188333333333333])\n"
     ]
    }
   ],
   "source": [
    "# prepare the model for training\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              learning_rate=0.1,\n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "print(model.evaluate(X_train_normalized, y_train_one_hot))\n",
    "# history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\tloss=2.3024\taccuracy=0.1119\n",
      "Epoch 2:\tloss=2.3024\taccuracy=0.1119\n",
      "Epoch 3:\tloss=2.3023\taccuracy=0.1119\n",
      "Epoch 4:\tloss=2.3023\taccuracy=0.1119\n",
      "Epoch 5:\tloss=2.3023\taccuracy=0.1119\n",
      "Epoch 6:\tloss=2.3023\taccuracy=0.1119\n",
      "Epoch 7:\tloss=2.3022\taccuracy=0.1119\n",
      "Epoch 8:\tloss=2.3022\taccuracy=0.1119\n",
      "Epoch 9:\tloss=2.3022\taccuracy=0.1119\n",
      "Epoch 10:\tloss=2.3022\taccuracy=0.1119\n",
      "Epoch 11:\tloss=2.3021\taccuracy=0.1119\n",
      "Epoch 12:\tloss=2.3021\taccuracy=0.1119\n",
      "Epoch 13:\tloss=2.3021\taccuracy=0.1119\n",
      "Epoch 14:\tloss=2.3021\taccuracy=0.1119\n",
      "Epoch 15:\tloss=2.3021\taccuracy=0.1119\n",
      "Epoch 16:\tloss=2.3020\taccuracy=0.1119\n",
      "Epoch 17:\tloss=2.3020\taccuracy=0.1119\n",
      "Epoch 18:\tloss=2.3020\taccuracy=0.1119\n",
      "Epoch 19:\tloss=2.3020\taccuracy=0.1119\n",
      "Epoch 20:\tloss=2.3020\taccuracy=0.1119\n",
      "Epoch 21:\tloss=2.3019\taccuracy=0.1119\n",
      "Epoch 22:\tloss=2.3019\taccuracy=0.1119\n",
      "Epoch 23:\tloss=2.3019\taccuracy=0.1119\n",
      "Epoch 24:\tloss=2.3019\taccuracy=0.1119\n",
      "Epoch 25:\tloss=2.3019\taccuracy=0.1119\n",
      "Epoch 26:\tloss=2.3018\taccuracy=0.1119\n",
      "Epoch 27:\tloss=2.3018\taccuracy=0.1119\n",
      "Epoch 28:\tloss=2.3018\taccuracy=0.1119\n",
      "Epoch 29:\tloss=2.3018\taccuracy=0.1119\n",
      "Epoch 30:\tloss=2.3018\taccuracy=0.1119\n",
      "Epoch 31:\tloss=2.3018\taccuracy=0.1119\n",
      "Epoch 32:\tloss=2.3017\taccuracy=0.1119\n",
      "Epoch 33:\tloss=2.3017\taccuracy=0.1119\n",
      "Epoch 34:\tloss=2.3017\taccuracy=0.1119\n",
      "Epoch 35:\tloss=2.3017\taccuracy=0.1119\n",
      "Epoch 36:\tloss=2.3017\taccuracy=0.1119\n",
      "Epoch 37:\tloss=2.3017\taccuracy=0.1119\n",
      "Epoch 38:\tloss=2.3017\taccuracy=0.1119\n",
      "Epoch 39:\tloss=2.3016\taccuracy=0.1119\n",
      "Epoch 40:\tloss=2.3016\taccuracy=0.1119\n",
      "Epoch 41:\tloss=2.3016\taccuracy=0.1119\n",
      "Epoch 42:\tloss=2.3016\taccuracy=0.1119\n",
      "Epoch 43:\tloss=2.3016\taccuracy=0.1119\n",
      "Epoch 44:\tloss=2.3016\taccuracy=0.1119\n",
      "Epoch 45:\tloss=2.3016\taccuracy=0.1119\n",
      "Epoch 46:\tloss=2.3015\taccuracy=0.1119\n",
      "Epoch 47:\tloss=2.3015\taccuracy=0.1119\n",
      "Epoch 48:\tloss=2.3015\taccuracy=0.1119\n",
      "Epoch 49:\tloss=2.3015\taccuracy=0.1119\n",
      "Epoch 50:\tloss=2.3015\taccuracy=0.1119\n",
      "Epoch 51:\tloss=2.3015\taccuracy=0.1119\n",
      "Epoch 52:\tloss=2.3015\taccuracy=0.1119\n",
      "Epoch 53:\tloss=2.3015\taccuracy=0.1119\n",
      "Epoch 54:\tloss=2.3014\taccuracy=0.1119\n",
      "Epoch 55:\tloss=2.3014\taccuracy=0.1119\n",
      "Epoch 56:\tloss=2.3014\taccuracy=0.1119\n",
      "Epoch 57:\tloss=2.3014\taccuracy=0.1119\n",
      "Epoch 58:\tloss=2.3014\taccuracy=0.1119\n",
      "Epoch 59:\tloss=2.3014\taccuracy=0.1119\n",
      "Epoch 60:\tloss=2.3014\taccuracy=0.1119\n",
      "Epoch 61:\tloss=2.3014\taccuracy=0.1119\n",
      "Epoch 62:\tloss=2.3014\taccuracy=0.1119\n",
      "Epoch 63:\tloss=2.3014\taccuracy=0.1119\n",
      "Epoch 64:\tloss=2.3013\taccuracy=0.1119\n",
      "Epoch 65:\tloss=2.3013\taccuracy=0.1119\n",
      "Epoch 66:\tloss=2.3013\taccuracy=0.1119\n",
      "Epoch 67:\tloss=2.3013\taccuracy=0.1119\n",
      "Epoch 68:\tloss=2.3013\taccuracy=0.1119\n",
      "Epoch 69:\tloss=2.3013\taccuracy=0.1119\n",
      "Epoch 70:\tloss=2.3013\taccuracy=0.1119\n",
      "Epoch 71:\tloss=2.3013\taccuracy=0.1119\n",
      "Epoch 72:\tloss=2.3013\taccuracy=0.1119\n",
      "Epoch 73:\tloss=2.3013\taccuracy=0.1119\n",
      "Epoch 74:\tloss=2.3013\taccuracy=0.1119\n"
     ]
    }
   ],
   "source": [
    "np.seterr(all='warn')\n",
    "\n",
    "# train the model\n",
    "model.fit(X_train_normalized, y_train_one_hot, epochs=200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99435567 0.99543325 0.9944459  ... 0.99484271 0.99415878 0.99443751]\n",
      " [0.99435567 0.99543325 0.9944459  ... 0.99484271 0.99415878 0.99443751]\n",
      " [0.99435567 0.99543325 0.9944459  ... 0.99484271 0.99415878 0.99443751]\n",
      " ...\n",
      " [0.99435567 0.99543325 0.9944459  ... 0.99484271 0.99415878 0.99443751]\n",
      " [0.99435567 0.99543325 0.9944459  ... 0.99484271 0.99415878 0.99443751]\n",
      " [0.99435567 0.99543325 0.9944459  ... 0.99484271 0.99415878 0.99443751]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moaz/Learn/AI/Machine Learning 2020/Deep Learning From Scratch/activations.py:14: RuntimeWarning: underflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.call(X_train)\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan nan nan nan nan nan nan nan nan nan]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print((model.layers[-1].cache['linear'])[0:5].sum(axis=0))\n",
    "# print(model.)\n",
    "print(y_train_one_hot[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116: 2.299627642908086\n",
      "0.085\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "Epoch 117: 2.2987160707056655\n",
      "0.089\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "Epoch 118: 2.297547054960231\n",
      "0.093\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "Epoch 119: 2.2969361877346177\n",
      "0.097\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "Epoch 120: 2.3013728923701002\n",
      "0.102\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "Epoch 121: 2.3316311661641502\n",
      "0.108\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "Epoch 122: 2.470877203179891\n",
      "0.115\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "Epoch 123: 2.9382200502740887\n",
      "0.124\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "Epoch 124: 4.071433705680639\n",
      "0.135\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "Epoch 125: 21.65176835997178\n",
      "0.151\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "Epoch 126: nan\n",
      "0.175\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "Epoch 127: nan\n",
      "nan\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "Epoch 128: nan\n",
      "nan\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "Epoch 129: nan\n",
      "nan\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "Epoch 130: nan\n",
      "nan\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "for i in range(115, 130):\n",
    "    print(f\"Epoch {i+1}: {history[i]['loss']}\")\n",
    "    print(history[i]['parameters'][1]['biases'].mean().round(3))\n",
    "    print(y_train_one_hot[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09979146, 0.11161951, 0.09737951, ..., 0.10534638, 0.09922102,\n",
       "        0.09857147],\n",
       "       [0.09987466, 0.11158282, 0.0972883 , ..., 0.10537053, 0.09927624,\n",
       "        0.0985752 ],\n",
       "       [0.10208017, 0.11050944, 0.09498945, ..., 0.10592384, 0.10069647,\n",
       "        0.09860435],\n",
       "       ...,\n",
       "       [0.10137038, 0.11085667, 0.09572133, ..., 0.10574528, 0.10024484,\n",
       "        0.09859454],\n",
       "       [0.09981856, 0.11160903, 0.09734868, ..., 0.1053593 , 0.09923641,\n",
       "        0.09856812],\n",
       "       [0.10040861, 0.11130731, 0.09672001, ..., 0.10551907, 0.09963273,\n",
       "        0.0985899 ]])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history[1-1]['parameters'][2]['activations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 2.3025 - accuracy: 0.0990\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 0s 3us/sample - loss: 2.3025 - accuracy: 0.0990\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3024 - accuracy: 0.0990\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3024 - accuracy: 0.0990\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3024 - accuracy: 0.0990\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3024 - accuracy: 0.0990\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3023 - accuracy: 0.0990\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3023 - accuracy: 0.0990\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3023 - accuracy: 0.0990\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3022 - accuracy: 0.0990\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3022 - accuracy: 0.0990\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3022 - accuracy: 0.0990\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3022 - accuracy: 0.1119\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3021 - accuracy: 0.1119\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3021 - accuracy: 0.1119\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3021 - accuracy: 0.1119\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3021 - accuracy: 0.1119\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3021 - accuracy: 0.1119\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3020 - accuracy: 0.1119\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3020 - accuracy: 0.1119\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3020 - accuracy: 0.1119\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3020 - accuracy: 0.1119\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3019 - accuracy: 0.1119\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3019 - accuracy: 0.1119\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3019 - accuracy: 0.1119\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3019 - accuracy: 0.1119\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3019 - accuracy: 0.1119\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3019 - accuracy: 0.1119\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3018 - accuracy: 0.1119\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3018 - accuracy: 0.1119\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3018 - accuracy: 0.1119\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3018 - accuracy: 0.1119\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3018 - accuracy: 0.1119\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3018 - accuracy: 0.1119\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3018 - accuracy: 0.1119\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3017 - accuracy: 0.1119\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3017 - accuracy: 0.1119\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3017 - accuracy: 0.1119\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3017 - accuracy: 0.1119\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3017 - accuracy: 0.1119\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3017 - accuracy: 0.1119\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3017 - accuracy: 0.1119\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3016 - accuracy: 0.1119\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3016 - accuracy: 0.1119\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3016 - accuracy: 0.1119\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3016 - accuracy: 0.1119\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3016 - accuracy: 0.1119\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3016 - accuracy: 0.1119\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3016 - accuracy: 0.1119\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3016 - accuracy: 0.1119\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3016 - accuracy: 0.1119\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3016 - accuracy: 0.1119\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3015 - accuracy: 0.1119\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3015 - accuracy: 0.1119\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3015 - accuracy: 0.1119\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3015 - accuracy: 0.1119\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3015 - accuracy: 0.1119\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3015 - accuracy: 0.1119\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3015 - accuracy: 0.1119\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3015 - accuracy: 0.1119\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3015 - accuracy: 0.1119\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3015 - accuracy: 0.1119\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3015 - accuracy: 0.1119\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3015 - accuracy: 0.1119\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3015 - accuracy: 0.1119\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3014 - accuracy: 0.1119\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3014 - accuracy: 0.1119\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3014 - accuracy: 0.1119\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3014 - accuracy: 0.1119\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3014 - accuracy: 0.1119\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3014 - accuracy: 0.1119\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3014 - accuracy: 0.1119\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3014 - accuracy: 0.1119\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3014 - accuracy: 0.1119\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3014 - accuracy: 0.1119\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3014 - accuracy: 0.1119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3014 - accuracy: 0.1119\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3014 - accuracy: 0.1119\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3014 - accuracy: 0.1119\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3014 - accuracy: 0.1119\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3014 - accuracy: 0.1119\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3014 - accuracy: 0.1119\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3014 - accuracy: 0.1119\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3014 - accuracy: 0.1119\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3014 - accuracy: 0.1119\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3013 - accuracy: 0.1119\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3013 - accuracy: 0.1119\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3013 - accuracy: 0.1119\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3013 - accuracy: 0.1119\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3013 - accuracy: 0.1119\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3013 - accuracy: 0.1119\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 0s 2us/sample - loss: 2.3013 - accuracy: 0.1119\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 2.3013 - accuracy: 0.1119\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 2.3013 - accuracy: 0.1119\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 2.3013 - accuracy: 0.1119\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 2.3013 - accuracy: 0.1119\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 2.3013 - accuracy: 0.1119\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 2.3013 - accuracy: 0.1119\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 2.3013 - accuracy: 0.1119\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 0s 1us/sample - loss: 2.3013 - accuracy: 0.1119\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe0ac4691d0>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "k_model = keras.Sequential([\n",
    "    keras.layers.Dense(16, input_shape=(784,), activation=\"sigmoid\", kernel_initializer=keras.initializers.RandomNormal(stddev=0.01)),\n",
    "    keras.layers.Dense(16, activation=\"sigmoid\", kernel_initializer=keras.initializers.RandomNormal(stddev=0.01)),\n",
    "    keras.layers.Dense(10, activation=\"sigmoid\", kernel_initializer=keras.initializers.RandomNormal(stddev=0.01))\n",
    "])\n",
    "\n",
    "k_model.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(lr=0.1),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "k_model.fit(X_train_normalized, y_train_one_hot, batch_size=X_train_normalized.shape[0], epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].call(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3367, 0.1288],\n",
      "        [0.2345, 0.2303]])\n",
      "My Dense Layer:\n",
      "Weights:\n",
      "[[ 0.00496714]\n",
      " [-0.00138264]]\n",
      "\n",
      "Biases:\n",
      "[[0.]]\n",
      "\n",
      "Activations:\n",
      "[[0.00149429]\n",
      " [0.00084614]]\n",
      "\n",
      "\n",
      "Torch Dense Layer:\n",
      "Weights:\n",
      "[[ 0.00496714 -0.00138264]]\n",
      "\n",
      "Biases:\n",
      "[[0.]]\n",
      "\n",
      "Activations:\n",
      "[[0.00149429]\n",
      " [0.00084614]]\n"
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "x = torch.randn(2, 2)\n",
    "print(x)\n",
    "\n",
    "my_dense = Dense(1, input_shape=x.shape[1:])\n",
    "my_dense.build(my_dense.input_shape)\n",
    "print(\"My Dense Layer:\")\n",
    "print(\"Weights:\")\n",
    "print(my_dense.weights)\n",
    "print()\n",
    "print(\"Biases:\")\n",
    "print(my_dense.biases)\n",
    "print()\n",
    "print(\"Activations:\")\n",
    "print(my_dense(x.numpy()))\n",
    "print()\n",
    "print()\n",
    "\n",
    "torch_dense = torch.nn.Linear(x.shape[1], 1)\n",
    "with torch.no_grad():\n",
    "    torch_dense.weight = torch.nn.Parameter(torch.tensor(my_dense.weights.T, dtype=torch.float))\n",
    "    torch_dense.bias = torch.nn.Parameter(torch.tensor(my_dense.biases, dtype=torch.float))\n",
    "\n",
    "    print(\"Torch Dense Layer:\")\n",
    "    print(\"Weights:\")\n",
    "    print(torch_dense.weight.numpy())\n",
    "    print()\n",
    "    print(\"Biases:\")\n",
    "    print(torch_dense.bias.numpy())\n",
    "    print()\n",
    "    print(\"Activations:\")\n",
    "    print(torch_dense(x).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000e+02, 2.0000e+02],\n",
      "        [4.0000e+02, 1.0000e+06]])\n",
      "My Dense Layer:\n",
      "Weights:\n",
      "[[ 0.49671415 -0.1382643   0.64768854]\n",
      " [ 1.52302986 -0.23415337 -0.23413696]]\n",
      "\n",
      "Biases:\n",
      "[[0. 0. 0.]]\n",
      "\n",
      "Activations:\n",
      "\n",
      "\n",
      "Torch Dense Layer:\n",
      "Weights:\n",
      "[[ 0.49671414  1.5230298 ]\n",
      " [-0.1382643  -0.23415338]\n",
      " [ 0.64768857 -0.23413695]]\n",
      "\n",
      "Biases:\n",
      "[[0. 0. 0.]]\n",
      "\n",
      "Linear Output:\n",
      "[[ 3.5427737e+02 -6.0657104e+01  1.7941467e+01]\n",
      " [ 1.5232284e+06 -2.3420869e+05 -2.3387788e+05]]\n",
      "\n",
      "Activations:\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# x = torch.randn(2, 2)\n",
    "x = torch.tensor([[100, 200],\n",
    "                  [400, 1000000]], dtype=torch.float)\n",
    "print(x)\n",
    "\n",
    "my_dense = Dense(3, input_shape=x.shape[1:], activation=\"softmax\")\n",
    "my_dense.build(my_dense.input_shape)\n",
    "my_dense.weights *= 100\n",
    "print(\"My Dense Layer:\")\n",
    "print(\"Weights:\")\n",
    "print(my_dense.weights)\n",
    "print()\n",
    "print(\"Biases:\")\n",
    "print(my_dense.biases)\n",
    "print()\n",
    "print(\"Activations:\")\n",
    "# print(my_dense(x.numpy()))\n",
    "print()\n",
    "print()\n",
    "\n",
    "torch_dense = torch.nn.Linear(x.shape[1], 1)\n",
    "torch_softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch_dense.weight = torch.nn.Parameter(torch.tensor(my_dense.weights.T, dtype=torch.float))\n",
    "    torch_dense.bias = torch.nn.Parameter(torch.tensor(my_dense.biases, dtype=torch.float))\n",
    "\n",
    "    print(\"Torch Dense Layer:\")\n",
    "    print(\"Weights:\")\n",
    "    print(torch_dense.weight.numpy())\n",
    "    print()\n",
    "    print(\"Biases:\")\n",
    "    print(torch_dense.bias.numpy())\n",
    "    print()\n",
    "    print(\"Linear Output:\")\n",
    "    print(torch_dense(x).numpy())\n",
    "    print()\n",
    "    print(\"Activations:\")\n",
    "    print(torch_softmax(torch_dense(x)).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(torch.tensor([[2, 4]], dtype=torch.float).softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09003057 0.24472847 0.66524096]\n",
      " [1.         0.         0.        ]\n",
      " [0.         0.         1.        ]]\n",
      "[[0.09003057 0.24472847 0.66524096]\n",
      " [1.         0.         0.        ]\n",
      " [0.         0.         1.        ]]\n",
      "[[0.73105858 0.88079708 0.95257413]\n",
      " [1.         1.         0.9999546 ]\n",
      " [1.         1.         1.        ]]\n",
      "[[0.73105858 0.88079708 0.95257413]\n",
      " [1.         1.         0.9999546 ]\n",
      " [1.         1.         1.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moaz/Learn/AI/Machine Learning 2020/Deep Learning From Scratch/activations.py:27: RuntimeWarning: underflow encountered in exp\n",
      "  exp_z = np.exp(z)\n",
      "/home/moaz/.local/lib/python3.7/site-packages/scipy/special/_logsumexp.py:112: RuntimeWarning: underflow encountered in exp\n",
      "  tmp = np.exp(a - a_max)\n",
      "/home/moaz/.local/lib/python3.7/site-packages/scipy/special/_logsumexp.py:215: RuntimeWarning: underflow encountered in exp\n",
      "  return np.exp(x - logsumexp(x, axis=axis, keepdims=True))\n",
      "/home/moaz/Learn/AI/Machine Learning 2020/Deep Learning From Scratch/activations.py:14: RuntimeWarning: underflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "from activations import Softmax, Sigmoid\n",
    "import scipy.special\n",
    "\n",
    "softmax = Softmax()\n",
    "sigmoid = Sigmoid()\n",
    "x = np.array([[1, 2, 3],\n",
    "              [1000, 100, 10],\n",
    "              [12345, 67890, 999999999]])\n",
    "\n",
    "np.seterr(all='warn')\n",
    "print(softmax(x))\n",
    "print(scipy.special.softmax(x, axis=-1))\n",
    "print(scipy.special.expit(x))\n",
    "print(sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6884189731172677\n",
      "tf.Tensor(2.302356719970703, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "from losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy as KCC\n",
    "\n",
    "loss_fn = CategoricalCrossentropy()\n",
    "k_loss_fn = KCC()\n",
    "y_pred = model.call(X_train)\n",
    "\n",
    "print(loss_fn(y_train_one_hot, y_pred))\n",
    "print(k_loss_fn(y_train_one_hot, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302563020449781"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = y_train_one_hot.shape[0]\n",
    "\n",
    "-np.sum(y_train_one_hot * np.log(y_pred / np.sum(y_pred, axis=-1, keepdims=True))) / m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python37364bit5171d068c3054b8f80dd7b7dde0249a4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
