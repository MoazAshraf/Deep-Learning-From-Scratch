{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# load the MNIST dataset\n",
    "mnist_data = fetch_openml('mnist_784')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "print(mnist_data['data'].shape)\n",
    "print(mnist_data['target'].shape)\n",
    "\n",
    "X = mnist_data['data']\n",
    "y = mnist_data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000,)\n",
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/7)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "index = 2\n",
    "\n",
    "# plot an example\n",
    "def plot_example(X, y):\n",
    "    plt.imshow(X.reshape(28, 28), cmap='binary')\n",
    "    plt.axis(False)\n",
    "    plt.title(f\"{y}\")\n",
    "\n",
    "plot_example(X_train[index], y_train[index])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot a few examples\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "for i in range(4):\n",
    "    fig.add_subplot(2, 2, i+1)\n",
    "    plot_example(X_train[i], y_train[i])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def one_hot_encode(x, num_classes=10):\n",
    "    x_one_hot = x.astype(int).reshape(-1)\n",
    "    x_one_hot = np.eye(num_classes)[x_one_hot]\n",
    "    return x_one_hot\n",
    "\n",
    "# one-hot encode labels\n",
    "y_train_one_hot = one_hot_encode(y_train)\n",
    "y_test_one_hot = one_hot_encode(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the inputs\n",
    "X_train_normalized = X_train / 255.0\n",
    "X_test_normalized = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Sequential\n",
    "from layers import Dense\n",
    "\n",
    "# create the model\n",
    "model = Sequential([\n",
    "    Dense(16, input_shape=(784,), activation=\"relu\"),\n",
    "#     Dense(16, activation=\"relu\"),\n",
    "    Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=2.3024\taccuracy=0.0979\n",
      "(2.302369439280033, [0.09793333333333333])\n"
     ]
    }
   ],
   "source": [
    "# prepare the model for training\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              learning_rate=0.1,\n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "print(model.evaluate(X_train_normalized, y_train_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\tloss=2.3022\taccuracy=0.0979\n",
      "Epoch 2:\tloss=2.3021\taccuracy=0.0979\n",
      "Epoch 3:\tloss=2.3019\taccuracy=0.0979\n",
      "Epoch 4:\tloss=2.3018\taccuracy=0.0979\n",
      "Epoch 5:\tloss=2.3016\taccuracy=0.0979\n",
      "Epoch 6:\tloss=2.3014\taccuracy=0.0979\n",
      "Epoch 7:\tloss=2.3012\taccuracy=0.0979\n",
      "Epoch 8:\tloss=2.3010\taccuracy=0.0979\n",
      "Epoch 9:\tloss=2.3008\taccuracy=0.0979\n",
      "Epoch 10:\tloss=2.3006\taccuracy=0.0979\n",
      "Epoch 11:\tloss=2.3003\taccuracy=0.0979\n",
      "Epoch 12:\tloss=2.3001\taccuracy=0.0979\n",
      "Epoch 13:\tloss=2.2998\taccuracy=0.0979\n",
      "Epoch 14:\tloss=2.2994\taccuracy=0.0979\n",
      "Epoch 15:\tloss=2.2990\taccuracy=0.0979\n",
      "Epoch 16:\tloss=2.2986\taccuracy=0.0979\n",
      "Epoch 17:\tloss=2.2980\taccuracy=0.0979\n",
      "Epoch 18:\tloss=2.2974\taccuracy=0.0979\n",
      "Epoch 19:\tloss=2.2965\taccuracy=0.0979\n",
      "Epoch 20:\tloss=2.2956\taccuracy=0.0979\n",
      "Epoch 21:\tloss=2.2944\taccuracy=0.0979\n",
      "Epoch 22:\tloss=2.2932\taccuracy=0.0979\n",
      "Epoch 23:\tloss=2.2922\taccuracy=0.0979\n",
      "Epoch 24:\tloss=2.2919\taccuracy=0.0979\n",
      "Epoch 25:\tloss=2.2934\taccuracy=0.0979\n",
      "Epoch 26:\tloss=2.2985\taccuracy=0.0979\n",
      "Epoch 27:\tloss=2.3103\taccuracy=0.0979\n",
      "Epoch 28:\tloss=2.3332\taccuracy=0.0979\n",
      "Epoch 29:\tloss=2.3721\taccuracy=0.0979\n",
      "Epoch 30:\tloss=2.4318\taccuracy=0.0979\n",
      "Epoch 31:\tloss=2.5167\taccuracy=0.0979\n",
      "Epoch 32:\tloss=2.6322\taccuracy=0.0979\n",
      "Epoch 33:\tloss=2.7834\taccuracy=0.0979\n",
      "Epoch 34:\tloss=2.9725\taccuracy=0.0979\n",
      "Epoch 35:\tloss=3.2035\taccuracy=0.0979\n",
      "Epoch 36:\tloss=3.4878\taccuracy=0.0979\n",
      "Epoch 37:\tloss=3.8432\taccuracy=0.0979\n",
      "Epoch 38:\tloss=4.2879\taccuracy=0.0979\n",
      "Epoch 39:\tloss=4.8668\taccuracy=0.0979\n",
      "Epoch 40:\tloss=5.9501\taccuracy=0.0979\n"
     ]
    }
   ],
   "source": [
    "np.seterr(all='raise')\n",
    "\n",
    "# train the model\n",
    "history = model.fit(X_train_normalized, y_train_one_hot, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.70216723215762"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history[39]['parameters'][1][\"weights\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].call(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3367, 0.1288],\n",
      "        [0.2345, 0.2303]])\n",
      "My Dense Layer:\n",
      "Weights:\n",
      "[[ 0.00496714]\n",
      " [-0.00138264]]\n",
      "\n",
      "Biases:\n",
      "[[0.]]\n",
      "\n",
      "Activations:\n",
      "[[0.00149429]\n",
      " [0.00084614]]\n",
      "\n",
      "\n",
      "Torch Dense Layer:\n",
      "Weights:\n",
      "[[ 0.00496714 -0.00138264]]\n",
      "\n",
      "Biases:\n",
      "[[0.]]\n",
      "\n",
      "Activations:\n",
      "[[0.00149429]\n",
      " [0.00084614]]\n"
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "x = torch.randn(2, 2)\n",
    "print(x)\n",
    "\n",
    "my_dense = Dense(1, input_shape=x.shape[1:])\n",
    "my_dense.build(my_dense.input_shape)\n",
    "print(\"My Dense Layer:\")\n",
    "print(\"Weights:\")\n",
    "print(my_dense.weights)\n",
    "print()\n",
    "print(\"Biases:\")\n",
    "print(my_dense.biases)\n",
    "print()\n",
    "print(\"Activations:\")\n",
    "print(my_dense(x.numpy()))\n",
    "print()\n",
    "print()\n",
    "\n",
    "torch_dense = torch.nn.Linear(x.shape[1], 1)\n",
    "with torch.no_grad():\n",
    "    torch_dense.weight = torch.nn.Parameter(torch.tensor(my_dense.weights.T, dtype=torch.float))\n",
    "    torch_dense.bias = torch.nn.Parameter(torch.tensor(my_dense.biases, dtype=torch.float))\n",
    "\n",
    "    print(\"Torch Dense Layer:\")\n",
    "    print(\"Weights:\")\n",
    "    print(torch_dense.weight.numpy())\n",
    "    print()\n",
    "    print(\"Biases:\")\n",
    "    print(torch_dense.bias.numpy())\n",
    "    print()\n",
    "    print(\"Activations:\")\n",
    "    print(torch_dense(x).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000e+02, 2.0000e+02],\n",
      "        [4.0000e+02, 1.0000e+06]])\n",
      "My Dense Layer:\n",
      "Weights:\n",
      "[[ 0.49671415 -0.1382643   0.64768854]\n",
      " [ 1.52302986 -0.23415337 -0.23413696]]\n",
      "\n",
      "Biases:\n",
      "[[0. 0. 0.]]\n",
      "\n",
      "Activations:\n",
      "\n",
      "\n",
      "Torch Dense Layer:\n",
      "Weights:\n",
      "[[ 0.49671414  1.5230298 ]\n",
      " [-0.1382643  -0.23415338]\n",
      " [ 0.64768857 -0.23413695]]\n",
      "\n",
      "Biases:\n",
      "[[0. 0. 0.]]\n",
      "\n",
      "Linear Output:\n",
      "[[ 3.5427737e+02 -6.0657104e+01  1.7941467e+01]\n",
      " [ 1.5232284e+06 -2.3420869e+05 -2.3387788e+05]]\n",
      "\n",
      "Activations:\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# x = torch.randn(2, 2)\n",
    "x = torch.tensor([[100, 200],\n",
    "                  [400, 1000000]], dtype=torch.float)\n",
    "print(x)\n",
    "\n",
    "my_dense = Dense(3, input_shape=x.shape[1:], activation=\"softmax\")\n",
    "my_dense.build(my_dense.input_shape)\n",
    "my_dense.weights *= 100\n",
    "print(\"My Dense Layer:\")\n",
    "print(\"Weights:\")\n",
    "print(my_dense.weights)\n",
    "print()\n",
    "print(\"Biases:\")\n",
    "print(my_dense.biases)\n",
    "print()\n",
    "print(\"Activations:\")\n",
    "# print(my_dense(x.numpy()))\n",
    "print()\n",
    "print()\n",
    "\n",
    "torch_dense = torch.nn.Linear(x.shape[1], 1)\n",
    "torch_softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch_dense.weight = torch.nn.Parameter(torch.tensor(my_dense.weights.T, dtype=torch.float))\n",
    "    torch_dense.bias = torch.nn.Parameter(torch.tensor(my_dense.biases, dtype=torch.float))\n",
    "\n",
    "    print(\"Torch Dense Layer:\")\n",
    "    print(\"Weights:\")\n",
    "    print(torch_dense.weight.numpy())\n",
    "    print()\n",
    "    print(\"Biases:\")\n",
    "    print(torch_dense.bias.numpy())\n",
    "    print()\n",
    "    print(\"Linear Output:\")\n",
    "    print(torch_dense(x).numpy())\n",
    "    print()\n",
    "    print(\"Activations:\")\n",
    "    print(torch_softmax(torch_dense(x)).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(torch.tensor([[2, 4]], dtype=torch.float).softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moaz/.local/lib/python3.7/site-packages/scipy/special/_logsumexp.py:112: RuntimeWarning: underflow encountered in exp\n",
      "  tmp = np.exp(a - a_max)\n",
      "/home/moaz/.local/lib/python3.7/site-packages/scipy/special/_logsumexp.py:215: RuntimeWarning: underflow encountered in exp\n",
      "  return np.exp(x - logsumexp(x, axis=axis, keepdims=True))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.09003057, 0.24472847, 0.66524096],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 1.        ]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from activations import Softmax\n",
    "import scipy.special\n",
    "\n",
    "softmax = Softmax()\n",
    "x = np.array([[1, 2, 3],\n",
    "              [1000, 100, 10],\n",
    "              [12345, 67890, 999999999]])\n",
    "\n",
    "np.seterr(all='warn')\n",
    "# print(softmax(x))\n",
    "scipy.special.softmax(x, axis=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python37364bit5171d068c3054b8f80dd7b7dde0249a4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
